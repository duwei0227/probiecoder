---
layout: post
title: XGBoost
categories: [机器学习，集成模型]
description: XGBoost
keywords: XGBoost
---
XGBoost

### 目标函数推导
目标： $Obj^{(t)} = \sum_{i=1}^nl\big(y_i, \hat{y_i}^{(t-1)} + f_t(x)\big) + \Omega(f_t) + constant$
l为损失函数；$\Omega$为正则项，包括L1、L2；constant为常数项  

使用泰勒展开(麦克劳林)近似目标解：  
* 泰勒展开公式 $\frac{f'(x)^n}{n!}x^n$ = $f(x + \Delta{x}) \simeq f(x) + f'(x)\Delta{x} + \frac{1}{2}f''(x)\Delta{x}^2$
* 定义： $g_i = \partial_{\hat{y}^{t-1}}l(y_i, \hat{y}^{(t-1)}), h_i=\partial_{\hat{y}^{t-1}}^2l(y_i, \hat{y}^{(t-1)})$

$$
Obj^{(t)} \simeq \sum_{i=1}^n\big[l(y_i, \hat{y_i}^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)\big] + \Omega(f_t) + constant
$$

对函数f细化，把树拆分为结构部分q和叶子权重部分w。得到：
$$ f_t(x) = w_{q(x)}， \quad w \in R^T, q: R^d \to {1, 2, \dots, T}$$

正则化项处理：  
$$ \Omega(f_t) = \gamma^T + \frac{1}{2}\lambda\sum_{j=1}^Tw_j^2 $$

对目标函数进行改写，其中I定义为每个叶子上面样本集合$I_j = \{i|q(x_i) = j\}$
最小化损失
$$ 
Obj^{(t)} \simeq \sum_{i=1}^n\big[g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i)\big] + \Omega(f_t) \\
= \sum_{i=1}^n\big[g_iw_{q(x_i)} + \frac{1}{2}h_iw_{q(x_i)}^2\big] + \gamma^T + \lambda\frac{1}{2}\sum_{j=1}^Tw_j^2 \\
= \sum_{j=1}^T\big[(\sum_{i\in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w_j^2\big] + \gamma^T
$$
这一个目标包含了T个相互独立的单变量二次函数。定义：
$$ G_j = \sum_{i \in I_j}g_i \quad H_j = \sum_{i \in I_j}h_i $$

最终简化为：
$$
Obj^{(t)} = \sum_{j=1}^T\big[(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_ + \lambda)w_j^2\big]+\gamma^T \\
\sum_{j=1}^T\big[G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2\big] + \gamma^T
$$
对$w_j$求导得到：  
$\partial w_j = -\frac{G_j}{H_j + \lambda}$  
带入得到：  
$$ Obj = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j + \lambda} + \gamma^T $$

参考：  
[https://blog.csdn.net/a819825294/article/details/51206410](https://blog.csdn.net/a819825294/article/details/51206410)