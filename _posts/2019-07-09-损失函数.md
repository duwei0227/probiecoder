---
layout: post
title: 损失函数
categories: [机器学习]
description: 损失函数
keywords: 损失函数
---
损失函数

&nbsp;&nbsp;用来评价模型输出的预测值$\hat{y} = f(X)$与真实值$Y$之间的差异。

### 一、回归损失函数

#### 1、均方误差损失函数(MSE)
无法处理梯度消失的问题，Mean Squared Error Loss  
定义：  
$$
loss(Y, \hat{Y}) = \frac{1}{N}\sum_{i=1}^N(\hat{y_i} - y_i)^2
$$  

示例代码：  
```python
def mean_squared_error(y_true, y_pred):
    """均方误差损失函数实现"""
    return np.mean(np.square(y_pred - y_true), axis=-1)
```  
缺点：  
* 对异常值非常敏感
* 平方操作会放大异常值

#### 2、平均绝对误差损失函数(MAE)
Mean Absolute Error Loss , 会比较有效地惩罚异常值    
定义：  
$$
loss(Y, \hat{Y}) = \frac{1}{N}\sum_{i=1}^N|\hat{y_i} - y_i|
$$  
代码示例：  
```python
def mean_absolute_error(y_true, y_pred):
    """平均绝对误差损失函数实现"""
    return np.mean(np.abs(y_pred - y_true), axis=-1)
```  

#### 3、均方误差对数损失函数(MSLE)
Mean Squared Log Error Loss，对每个输出数据进行对数计算，缩小函数输出值的范围值  
定义：  
$$
loss(Y, \hat{Y}) = \frac{1}{N}\sum_{i=1}^N(\log\hat{y_i} - \log{y_i})^2
$$  
代码示例：  
```python
def mean_squared_logarithmic_error(y_true, y_pred):
    """均方误差对数损失函数"""
    first_log = np.log(np.clip(y_pred, 10e-6, None) + 1.)
    second_log = np.log(np.clip(y_true, 10e-6, None) + 1.)
    return np.mean(np.square(first_log, second_log), axis=-1)
```  
设计对数运算，为了避免计算log0没有意义，因此加入一个很小的常数$\epsilon=10^{-6}$作为计算补偿

#### 4、平均绝对百分比误差损失函数(MAPE)
Mean Absolute Percentage Error Loss  
定义：  
$$
loss(Y, \hat{Y}) = \frac{1}{N}\sum_{i=1}^N\frac{100 \times |\hat{y_i} - y_i|}{y_i}
$$  
代码示例：  
```python
def mean_absolute_percentage_error(y_true, y_pred):
    """平均绝对百分比误差损失函数实现"""
    diff = np.abs((y_pred - y_true) / np.clip(np.abs(y_true), 10e-6, None))
    return 100 * np.mean(diff, axis=-1)
```

### 二、分类损失函数
#### 1、Logistic损失函数
在最大似然估计函数中，定义一个损失函数为$loss(Y, P(Y|X)$，公式表示样本X在分类 Y的情况下，使概率$P(Y|X)$达到最大值。    
定义：  
$$ 
loss(Y, \hat{Y}) = \prod_{i-0}^N{\hat{y_i} \times (1 - \hat{y_i})^{1-y_i}}
$$  

#### 2、负对数似然损失函数
为方便数学运算，在处理概率乘积时通常把最大似然函数转化为概率的对数，可以将连乘转为求和。在前面加一个负号之后，最大化概率等价于寻找最小化的损失。    
定义：  
$$ 
loss(Y, \hat{Y}) = -\sum_{i=0}^N{y_i \times \log{\hat{y_i}} + (1 - y_i) \times \log{(1 - \hat{y_i})}}
$$

#### 3、交叉熵损失函数
Cross Entropy Loss 处理多分类   
定义：    
$$
loss(Y, \hat{Y}) = -\sum_{i=1}^N\sum_{j=1}^My_{ij} \times \log{\hat{y_{ij}}}
$$    
代码示例：  
```python
def cross_entropy(y_true, y_pred):
    """交叉熵损失函数实现"""
    return -np.sum(y_true * np.log(y_pred + 10e-6))
```

#### 4、Hinge损失函数
可以解决间隔最大化问题。当分类模型需要硬分类结果，例如分类结果是0或1，-1或1的二分类数据  
使用示例：SVM    
定义：  
$$
loss(Y, \hat{Y}) = \frac{1}{N}\sum_{i=1}^Nmax(0, 1 - \hat{y} \times y_i)
$$  
代码示例：  
```python
def hinge(y_true, y_pred):
    return np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)
```

#### 5、指数损失函数
使用示例：AdaBoost  
定义：  
$$
loss(Y, \hat{Y}) = \sum_{i=1}^Ne^{-y_i \times \hat{y_i}}
$$  
代码示例：  
```python
def exponential(y_true, y_pred):
    return np.sum(np.exp(-y_true * y_pred))
```
