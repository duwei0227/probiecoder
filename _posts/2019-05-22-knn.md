---
layout: post
title: K Nearest Neighbour
categories: [机器学习, 监督学习, 分类]
description: K Nearest Neighbour
keywords: KNN, K Nearest Neighbour, K近邻
---
K Nearest Neighbour

### KNN
三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的$L_p$距离。k值的选择反映了对近似误差与估计误差之间的权衡。
**算法：** 给定一个点$x_0$,在训练集中找到K个最近邻的点，根据这K个点的分类来决定$x_0$的类别(少数服从多数) 
$$ y = argmax_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j)$$ 
$N_k$:涵盖这k个点的x的邻域  
I为指数函数，即当$y_i = c_j$时I为1， 否则I为0  
误分类率：  
$$ \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1 - \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j) $$
**度量：** 如果特征是连续的，选择Euclidean, Manhattan or Minkowski 度量；如果特征是分类变量，选择Hamming度量  
**K的选择：** 如果是2分类问题，K一般选取奇数。K值越小，噪音对结果的影响越大，整体模型变得复杂，容易发生过拟合；K值越大，计算量越大，甚至可能导致分类完全错误。交叉验证是选取K值的有效方法。  
**优点：**
* 无变量算法，无需对数据的分布做任何假设
* K值固定时，无需训练模型
* 依赖局部信息，适应各种数据复杂分布
* 算法简单易实现

**缺点：** 
* 应用算法之前，需对所有特征做标准化处理 $x'=\frac{x-x_{min}}{x_{max}-x_{min}}$
* 计算时需要很大的内存
* 计算时间复杂度高
* 易于受维度灾难的影响
* K值的选取会影响模型结果

### kd树
一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分。相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个节点对应一个k维超矩形区域。  
#### 构造平衡kd树： 
输入：k维空间数据集$T=\{x_1, x_2, \cdots, x_n\}$  
输出：kd树  
（1）开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域。 
选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根节点对应的超矩形区域划分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现  
$\quad$由根节点生成深度为1的左、右子节点：左子节点对应坐标$x^{(1)}$小于切分的子区域，右子节点对应于坐标$x^{(1)}$大于切分点的子区域。  
  将落在切分超平面上的实例点保存在根节点。
（2）重复：对深度为j的节点，选择$x^{(1)}$为切分的坐标轴，$l=j(modk)+1$，以该节点的区域中所有实例的$x^{(1)}$坐标的中位数为切分点，将该节点对应的超矩形区域划分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。  
$\quad$由该节点生成深度为j+1的左、右子节点：左子节点对应坐标$x^{(1)}$小于切分的子区域，右子节点对应于坐标$x^{(1)}$大于切分点的子区域。  
（3）直到两个子区域没有实例存在时停止。从而形成kd树的区域划分  

#### 用kd树的最近邻搜索
输入：已构造的kd树；目标点x  
输出：x的最近邻  
（1）在kd树种找出包含目标点x的叶节点：从根节点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到子节点为叶节点为止。  
（2）以此叶节点为“当前最近点”  
（3）递归地向上回退，在每个节点进行以下操作：  
（a）如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。  
（b）当前最近点一定存在于该节点一个子节点对应的区域。检查该子节点的父节点的另一个子节点对应的区域是否有更近的点。具体地，检查另一个子节点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交 。  
 $\quad$如果相交，可能在另一个子节点对应的区域内存在距目标点更近的点，移动到另一个子节点。接着，递归地进行最近邻搜索；  
$\quad$如果不相交，向上回退。  
（4）当回退到根节点时，搜索结束，最后的“当前最近点”即为x的最近邻点。  
如果实例点是随机分布的，kd树搜索的平均计算复杂度是$O(logN)$。kd树更适合用于训练实例数远大于空间维数时的k近邻搜索。


#### 简单代码实例
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

x, y = make_blobs(n_samples=200, centers=2, n_features=2)

plt.scatter(x[:,0], x[:, 1], c=y)

model = KNeighborsClassifier(n_neighbors=3)
model.fit(x, y)
model.predict([[2, 3]])
```