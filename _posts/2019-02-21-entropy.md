---
layout: post
title: 熵的简单理解
categories: [机器学习]
description: 熵的简单理解
keywords: 机器学习，熵
---

熵知识点记录

## 熵
定义：表示随机变量不确定性的度量  
设X是一个取有限个值的离散随机变量，其概率分布为
$$ P(X = x_i) = p_i, \quad\quad i=1,2,\cdots,n $$
则随机变量X的熵定义为：
$$ H(X) = -\sum_{i=1}^{n}p_{i}\log{p_i} $$
通常对数以2为底或以e为底，这时熵定单位分别称作比特(bit)或纳特(nat)  
由定义可知，熵只依赖于X的分布，而与X的取值无关  
熵越大，随机变量的不确定性越大：$ 0\le H(p)\le\log{n} $


## 条件熵
定义：表示在已知随机变量X的条件下随机变量y的不确定性
$$ H(Y|X) = \sum_{i=1}^{n}p_iH(Y|X = x_i) $$
这里$p_i = P(X = x_i), \quad\quad i=1,2,\cdots,n$为X的分布概率  
ps：特征各取值的经验概率 * 划分子集的熵

## 信息增益
特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即
$$ g(D,A) = H(D) - H(D|A) $$
一般的，熵与条件熵之差称为互信息(mutual information)  

## 信息增益的算法
设训练集为D，|D|表示其样本容量，即样本个数。设有K个类$C_k,k=1,2,\cdots,K,|C_k|$为属于类$C_k$的样本个数。根据
特征A的取值将D划分为n个子集$D_1,D_2,\cdots,D_i,|D_i|为样本个数$  
输入：训练数据集D和特征A；  
输出：特征A对训练数据集D的信息增益g(D,A)  
（1）计算数据集D的经验熵H(D)
$$ H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log{\frac{|C_k|}{|D|}} $$
（2）计算特征A对数据集D的经验条件熵H(D|A)
$$  H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i}\log{\frac{|D_{ik}|}{|D_i|}} $$
（3）计算信息增益
$$ g(D,A) = H(D) - H(D|A) $$

## 信息增益比
定义：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即
$$ g_R(D, A) = \frac{g(D, A)}{H_A(D)} $$
其中，$H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log{\frac{|D_i|}{|D|}}$

> 参考 《统计学习》李航