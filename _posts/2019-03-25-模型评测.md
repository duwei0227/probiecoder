---
layout: post
title: 分类模型的评测
categories: [机器学习]
description: 分类模型的评测
keywords: 模型, 评测, 分类
---

分类模型的评测

### 1、模型的比较
N次训练中模型的比较  
不同算法的模型的比较  

### 2、评测集
评测集不混入训练集训练  
评测集只能代表片面的数据评测  
评测集需要数量和数据分布方面的扩充  

### 3、样本分类
通常需要判定概率为1的类型的样本叫做正样本(positive)  
通常需要判定概率为0的类型的样本叫做负样本(negative)  
使用超二分类可以扩充到多分类  

### 4、二分类评测指标
* Classification Accuracy
* Confusion Matrix
* ROC Curve
* Area under Curve
* F1 Score
* PR Curve
* AP Score

#### 4.1、Accuracy准确率
准确率/正确率  
在所有预测中，与正确答案相等的比例，包含正负样本
$$ Accuracy = \frac{Number \ of \ Correct \ Predictions}{Total \ number \ of \ predictions \ made} $$
使用混淆矩阵可以表示为: $\frac{TP + TN}{TP + TN + FP + FN}$  
缺陷：  
当数据的训练集和评测集的类型比例不平衡的时候，难以准确衡量模型的好坏  

例如：  
在训练集中，当A类型有98%的数据量，B类型有2%的数据量的时候，准确率可以达到98%  
当评测集中60%的数据是A，40%的数据是B的时候，测试的准确率会降到60%

#### 4.2、Confusion Matrix 混淆矩阵
对于二分类有四种预测结果：  
针对预测来说：真正(TP), 真负(TN),假正(FP),假负(FN)  

|         |        | 预测 |        |
| ------- | ------ | ------ | ----- |
|     |   | 1                 | 0|
| 实际 | 1 | True Positive(TP) | False Negative(FN) |
|     | 0 | False Positive(FP) | True Negative(TN) |

#### 4.3、Precision
查准率，也叫精确率或精度，所有预测为类别c的样本中，预测正确的比例  
预测label的准确率
$$ Precision = \frac{TP}{TP + FP} $$
$$ 误报率 = 1 - Precision $$
就预测y=1这件事情的正确率  

#### 4.4、Recall
查全率、召回率，所有真实标签为类别c的样本中，预测正确的比例    
$$ Recall = \frac{TP}{TP + FN} $$
$$ 漏报率 = 1 - Recall $$
对于所有y=1的数据有多少真的被报出来了

#### 4.5、阈值和Precision和Recall关系
阈值越低，Recall越高，Precision越低  
阈值越高，Recall越低，Precision越高

#### 4.6、对比ROC和PR
ROC：receiver operating characteristic curve  
* ROC永远是单调递增的 
* 正负样本失衡的时候，PR变化大而ROC可以保持不变
* 

#### 4.7、F-Score
$$ F_Score = \frac{2PR}{P + R} $$

#### 4.8、Micro(微平均)
每一个样本的性能指标的算术平均  
$$ Micro\_Precision = \sum_i{\frac{TP}{TP + FP}} $$
$$ Micro\_Recall = \sum_i{\frac{TP}{TP + FN}} $$
$$ Micro\_F\_Score = \frac{2Micro_Precition * Micro_Recall}{Micro_Precision + Micro_Recall} $$

#### 4.9、Macro(宏平均)
每一类的性能指标的算术平均值  
$$ Macro_Precision = \frac{1}{n}\sum_i^n{Precision_i} $$
$$ Macro_Recall = \frac{1}{n}\sum_i^n{Recall_i} $$

#### 4.10、mAP
mean(Api)
