---
layout: post
title: 深度学习常用激活函数
categories: [深度学习]
description: 深度学习常用激活函数
keywords: 深度学习，激活函数
---

深度学习常用激活函数

作用：
前后层通过线性连接，而线性函数具有可传递性，避免输入和输出层简单线性相连，增加一个非线性激活函数。

### sigmoid:  
$$ \sigma(x) = \frac{1}{1 + e^{-x}}$$
优点：
* 平滑函数-可求导
* 数据可压缩
* 适于前向传播


缺点：
* 梯度消失
* 不以 (0, 0)为中心，收敛速度慢
* $e^{-x}$计算速度慢
![Sigmoid](/images/Sigmoid.png)

### tanh:  
$$ \sigma(x)  = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

优点：
* 以(0, 0)为中心

缺点：
* 梯度消失
* e^x 计算速度慢

tanh和sigmiod联系：
$$ tanh(x) = 2 * sigmoid(2x) - 1 $$

![Tanh](/images/Tanh.png)

### ReLu
$$ ReLu = max(0, x) $$
优点：
* 解决梯度消失问题
* 收敛速度快 > sigmoid/tanh
* 没有指数运算，计算效率高
* 适合于反向传播

缺点：
* 不以(0, 0)为中心
* 不能压缩数据幅度，数据服务会随着模型层数不断增加

![ReLu](/images/ReLu.png)

### Leakly ReLu
$$ \sigma(x) = max(0.01x, x) $$

![Leakly_ReLu](/images/Leakly_ReLu.png)


### ELu
$$ \sigma(x) = \begin{cases}
    x \quad x \ge 0 \\
    e^x - 1 \quad x \lt 0
\end{cases} $$

![ELu](/images/ELu.png)

### PReLU
带参数的ReLU，引入一个可学习的参数，不同神经元有不同的参数
$$
PReLU_i(x) = \begin{cases}
    x \quad if x \gt 0 \\
    \gamma_ix \quad if x \le 0
\end{cases} 
 = max(0, x) + \gamma_imin(0, x)
$$

