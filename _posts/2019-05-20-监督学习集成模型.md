---
layout: post
title: 监督学习集成模型
categories: [机器学习]
description: 监督学习集成模型
keywords: 监督学习，集成，bagging，boosting
---

监督学习集成模型

## Bagging
### 装袋算法(Bagging)
一种提高分类模型的方法。  
步骤：  
1）从训练集S中**有放回**的**随机**选取数据集M($|M| \lt |S|$)  
2）生成一个分类模型  
3）重复以上步骤m次，得到m个分类模型$C_1, C_2,\dots,C_n$  
4）对于分类问题，每一个模型投票决定，少数服从多数；对于回归问题，取平均值  
优点：  
通过减少方差来提高预测结果  
缺点：  
失去了模型的简单性  

### Random Forest  
是一种基于树模型的装袋算法改进型。假定数据集中有M个特征和N个观测值。每一个树**有放回**的**随机**抽取出N个观测值，m($m=\sqrt{M}或者m=\log{M}$)个特征。把每一个单一决策树的结果综合起来。  
优点：  
* 减小了模型方差，提高了预测准确性
* 不需要给树剪枝
* 在大规模数据集，尤其是在特征较多的情况下，依然可以保持高效率  
* 不用做特征选择，并且可以给出特征变量重要性的排序估计

### 随机森林和装袋算法的区别
随机森林|装袋算法
-|-
单一模型是决策树 | 可任意单一模型
每一个决策树有放回的随机抽取数据，数据总数不变 | 每一个单一模型有放回的随机抽取数据，数据总数据减少（子集）
每一个决策树选取部分特征 | 每一个单一模型使用全部特征
模型结果更好 | 模型结果有所提高


## Boosting
### AdaBoost
在业界使用中获得成功的第一种提升模型。弱学习单一模型是决策树。在每一步的时候一次加入一个弱学习，集中学习上一步难以得到明显分类的数据集中。

### Gradient Boosting Decision Tree (GBDT)
Gradinet Boosting: 使用梯度下降法来实现的提升模型  
**优化函数：** 可自己定义，但必须是可导的。如果是回归问题，优化函数一般是 squared error ； 分类问题，优化函数一般是 log loss  
**弱学习模型：** 一般选举决策树  
**加性模型：** 每次添加一个弱学习模型，同时已生成的弱学习模型保持不变。使用梯度下降法去优化每次添加的弱学习模型。  
优点：  
* 通过减少模型偏差来提高准确性 
* 快速训练，计算效率高

缺点：  
* 容易过拟合 
* 难以并行计算

减小过拟合：
* 控制决策树的数量，树的深度，节点的个数，每次分隔节点的条件
* 给叶子的权重加正则化

### XGBoost
基于决策树的提升算法  
目标函数：
$$ Obj = \sum_{i=1}^nl(y_i, \hat{y_i}) + \sum_{k=1}^K\Omega(f_k) $$
目标函数第一部分为loss函数，第二部分为正则化函数  
优点：
* 计算效率高，使用了二阶导数
* 减少了过拟合，有正则化
* 列特征抽样(不同决策树特征不一样，关联程度较低)减少过拟合，同时有利于并行计算  

缺点：
* 每次迭代时，都要遍历整个数据集
* 内存占用较大

### LightGBM
基于决策树的提升算法。是基于XGBoost的一种改进
XGBoost和LightGBM的区别：  
XGBoost|LightGBM
-|-
pre-sorted方法 | histogram算法
level-wise生长策略 | leaf-wise生长策略

**Histogram算法：** 
* 把连续的浮点特征值离散化成N个整数，构造一个宽度为N的直方图
* 遍历数据时，根据离散化后的值作为索引在直方图中累积统计量
* 一次遍历后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点

**level-wise：** 逐层生成  
**leaf-wise：** 每一个叶子节点生长结束，在生长其他叶子节点  

## Voting
使用多种不同的模型作为学习模型，然后综合每种不同模型的结果得到结论。对于分类问题，一般是每种模型投票决定；对于回归问题，一般是取平均值。  

## Stacking
将不同的弱模型stack

Bagging，Boosting，Stacking比较：  
$\quad$| Bagging|Boosting|Stacking
-|-|-|-
数据集分隔 | 随机生成子集 | 随机生成全集 | 全集
效果 | 减少模型方差 | 减少模型偏差 | 都有可能，取决于选取的弱模型
