---
layout: post
title: 数据预处理
categories: [机器学习, 预处理]
description: 数据预处理
keywords: 数据预处理
---
数据预处理

**目的：** 使得数据分布无偏低方差  
#### 1、0均值(Zero Centralization)
用数据中的每一维度的数据值减去所在维度的数据均值  
对于图像数据可以简单地用所有像素值减去同一个值，也可以对彩色图像RGB的3个通道分别进行0均值操作  
操作顺序：首先对训练集的数据求均值，然后分别在验证和测试时减去训练集的均值，而不是减去所有或者自身的均值  

#### 2、归一化(Normalization)
把数据归一化到相同的尺度上  
1、将0均值后的数据的每一维除以每一维的标准差  
2、将数据中的每一维度归一化到区间[a, b],只适用于数据的不同维度应该具有相同的重要性时

#### 3、主成分分析(Principal Component Analysis, PCA)
寻找有效表示数据主轴的方向  
通过线性变换将原始数据变换为一组各维度线性无关的数据，可用于提取数据的主要特征分量，常用于高维数据的降维    
求解过程：  
1、将数据进行0均值处理  
2、计算协方差举证，得到数据不同维度之间的相关性  
3、对协方差举证进行SVD分解，得到3个矩阵  
代码示例：  
```python
def pca(X):
    x -= np.mean(X, axis=0)  # 对数据的数据X进行0均值计算
    COV = np.dot(x.T, X) / X.shape[0]  # 获取数据矩阵X的协方差矩阵
    U, S, V = np.linalg.svd(COV)  # 协方差矩阵SVD分解
    Xrot = np.dot(X, U)  # 数据去相关性
    PCA = np.dot(Xrot, U[:, :100])  # 获得去相关新的前100列数据
```  

#### 4、白化 (Whitening)
降低数据的冗余性，通过白化操作希望得到如下性质：  
* 特征之间相关性较低
* 所有特征具有相同的方差  

代码示例：  
```python
Xwhite = PCA / np.sqrt(S + 1e-6)
```