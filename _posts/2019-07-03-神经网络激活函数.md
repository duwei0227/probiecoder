---
layout: post
title: 激活函数
categories: [激活函数,深度学习]
description: 激活函数
keywords: 激活函数
---
激活函数

### 一、性质
一个没有激活函数的神经网络只不过是一个线性回归模型，不能表达复杂的数据分布
1) 单调可微
    一般情况下，我们使用梯度下降算法更新神经网络中的参数，因此必须要求激活函数可微。如果函数是单调递增的，求导后函数比大于零(方便计算)，因此需要激活函数具有单调性。
2) 限制输出值的范围
   输入的数据通过神经元上的激活函数来控制输出数值的大小，该输出数值是一个非线性值。通过激活函数得到的值，根据极限值来判断是否需要激活该神经元。
3) 非线性
   因为线性模型的表达能力不够（从数据输入到与权值求和加偏置，都是在线性函数内执行权重与输入数据进行加权求和的过程）

### 二、函数
#### 1、线性函数
表达式：
$$f(x) = ax + b$$
代码示例：
```python
def linear(x, a, b):
    return a * x + b
```
图形表示：  
![Linear](/images/Linear.png)

#### 2、Sigmoid函数
表达式：
$$
s(x) = \frac{1}{1 + e^{-ax}}
$$
代码示例：
```python
def sigmoid(x, w=1):
    return 1 / (1 + np.sum(np.exp(-wx)))
```
图形表示：  
![Sigmoid](/images/Sigmoid.png)  
优点：  
* Sigmoid函数的输出映射在[0, 1]范围内，函数单调连续，且输出范围有限制，优化稳定
* 易于求导 $s'(x) = s(x)(1-s(x))$
* 输出值为独立概率，可以用在输出层

缺点：
* Sigmoid函数容易饱和，导致训练结果不佳
* 其输出不是零均值，数据存在偏差，分布不平均

#### 3、Tanh函数(双曲正切)
表达式：
$$
tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$
代码示例：  
```python
def tanh(x):
    return np.tanh(x)
```
图形表示：  
![Tanh](/images/Tanh.png)  

优点：
* Tanh函数比Sigmoi函数收敛速度更快，更加易于训练
* 其输出以0为中心，数据分布均匀
缺点：
* 没有改变Sigmoid函数有饱和性质引起的梯度消失问题

#### 4、ReLU函数
表达式：
$$
relu(x) = max(0, x)
$$
代码示例：
```python
def relu(x):
    return x if x > 0 else 0
```
图形表示：  
![ReLu](/images/ReLu.png)  
![Leak ReLu](/images/Leakly_ReLu.png)
对比Sigmoid的变化：  
* 单侧抑制
* 相对宽阔的兴奋边界
* 稀疏激活性

优点：
* 相比Sigmoid和Tanh函数，在随机梯度下降算法中能够更快收敛
* 梯度为0或常数，因此可以缓解梯度消散的问题
* 引入稀疏激活性，在无监督预训练时也能有较好的表现

缺点：
* ReLU神经元在训练中不可逆的死亡
* 随着训练的进行，可能会出现神经元死亡、权重无法更新的现象，流经神经元的梯度从该点开始讲永远是零

#### 5、Softmax函数
用于处理多分类问题，会返回输出类的互斥概率分布，常作为输出层的激活函数
表达式：
$$
softmax(x) = \frac{e^{x_j}}{\sum_{k=1}^Ke^{x_k}}
$$
代码表示：
```python
def softmax(x):
    return np.exp(x) / np.sum(np.exp(x))
```
图形表示：  
![Softmax](/images/Softmax.png)
